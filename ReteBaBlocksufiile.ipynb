{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb88b10-71cf-49e0-b9b1-eb44c12fe7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, K2Score, HillClimbSearch\n",
    "from pgmpy.readwrite import BIFWriter\n",
    "import numpy as np\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7041a-9357-4992-b329-2e7d6637fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricare il dataset\n",
    "df3 = pd.read_csv('df_tipo1.csv')\n",
    "\n",
    "#print(df3)\n",
    "#df3 = df3.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "df3.replace({7: -1, 9: -1, 77 : -1, 99 : -1, 777: -1, 999: -1, 7777: -1, 9999: -1 ,'nan': -1}, inplace=True)\n",
    "df3.replace(-1.0, np.nan, inplace=True)\n",
    "# Contare i NaN per colonna\n",
    "nan_counts = df3.isna().sum()\n",
    "#nan_count_col1 = df3['DIABTYPE'].isna().sum()\n",
    "#print(f'Numero di NaN in col1: {nan_count_col1}')\n",
    "print(df3.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25595f95-68c6-4ad1-94ea-94cba9ed90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinare la soglia\n",
    "soglia = 120000  # Modificare questo valore in base alla soglia desiderata\n",
    "\n",
    "# Se il numero di valori mancanti in una colonna è maggiore della soglia, eliminare la colonna\n",
    "df_cleaned = df3.drop(columns=nan_counts[nan_counts > soglia].index)\n",
    "df_cleaned = df_cleaned.dropna(thresh=df_cleaned.shape[1] - 0)\n",
    "df_sampled = df_cleaned.astype({col: 'float32' for col in df_cleaned.select_dtypes(include='float64').columns})\n",
    "\n",
    "#df_sampled = df_cleaned.replace(np.nan, -1.0)\n",
    "print(df_sampled.info())\n",
    "print(df_sampled)\n",
    "#df_sampled = df_sampled.drop(columns = ['SMOKE100','INCOME3','_AGEG5YR.1', 'EDUCA'])\n",
    "#rint(df_sampled.dtypes)\n",
    "\n",
    "#df3 = df3.drop(columns=['LANDSEX1','COVIDINT', 'PFPPRVN4','PFPPRVN4','MARJEAT', 'HPVADSHT','BLDSTFIT','CDHELP'])\n",
    "#df3 = df3.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a10a55-a9fb-454c-8f93-920b3d94e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividere il dataset in train e test\n",
    "train_data, test_data = train_test_split(df_sampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b91fc-4ea8-4006-9325-eaf8fd665931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Definire la struttura della rete bayesiana\n",
    "# Utilizzare Hill-Climbing per apprendere la struttura\n",
    "hc = HillClimbSearch(df_sampled)\n",
    "best_model_structure = hc.estimate(max_iter = 100000, scoring_method=K2Score(df_sampled), max_indegree = 2)\n",
    "print(\"Archi della struttura appresa:\", best_model_structure.edges())\n",
    "\n",
    "# Verifica che best_model.edges() non sia None\n",
    "if best_model_structure.edges() is None:\n",
    "    raise ValueError(\"La struttura della rete appresa è vuota.\")\n",
    "# Stampa la struttura appresa\n",
    "#print(\"Archi della struttura appresa:\", best_model.edges())\n",
    "\n",
    "# Creare il modello con la struttura appresa\n",
    "try:\n",
    "    model = BayesianNetwork(best_model_structure.edges())\n",
    "except Exception as e:\n",
    "    print(f'Errore nella creazione del modello di rete bayesiana: {e}')\n",
    "    raise\n",
    "print(model)\n",
    "print(\"modello creato\")\n",
    "\n",
    "# Verifica che il modello sia stato creato correttamente\n",
    "if model is None:\n",
    "    raise ValueError(\"Errore nella creazione del modello di rete bayesiana.\")\n",
    "\n",
    "# Addestrare la rete bayesiana con i dati di training\n",
    "try:\n",
    "    model.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "except Exception as e:\n",
    "    print(f'Errore durante l\\'addestramento del modello: {e}')\n",
    "    raise\n",
    "\n",
    "# Stampa delle CPD\n",
    "#for cpd in model.get_cpds():\n",
    " #   print(cpd)\n",
    "# Salvare il modello addestrato in formato .bif\n",
    "#writer = BIFWriter(model)\n",
    "#writer.write_bif(filename='trained_network.bif')\n",
    "\n",
    "\n",
    "# Salvare i dati di test per utilizzarli successivamente\n",
    "#test_data.to_csv('path_to_save_test_data.csv', index=False)\n",
    "\n",
    "print(\"Modello addestrato e salvato con successo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13a4e8-c6e8-4ef3-b820-1ceb6c89a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_sampled.columns.tolist()\n",
    "model_nodes = set(model.nodes())\n",
    "presenti = [feature for feature in features if feature in model_nodes]\n",
    "non_presenti = [feature for feature in features if feature not in model_nodes]\n",
    "\n",
    "print(presenti)\n",
    "print(non_presenti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2aa36d-78f4-483c-8532-897ef489391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indici\n",
    "df_test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b431b9-333b-4f67-898f-59b1b0e45051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Creare un oggetto per l'inferenza\n",
    "inference = VariableElimination(model)\n",
    "\n",
    "# Previsione sul test set\n",
    "y_true = df_test_data['DIABTYPE1'].values\n",
    "y_pred = []\n",
    "\n",
    "df_test_data.drop(columns=['DIABTYPE1'], inplace=True)\n",
    "\n",
    "# Funzione per salvare le previsioni su file\n",
    "def salva_previsioni(y_pred, blocco):\n",
    "    print(f'Saving: {blocco}')\n",
    "    with open(f'previsioni_blocco_{blocco}.txt', 'w') as file:\n",
    "        for pred in y_pred:\n",
    "            file.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032708f6-cf4c-4d27-9d00-c323d9d753da",
   "metadata": {},
   "outputs": [],
   "source": [
    "stati_validi = {}\n",
    "for node in model.nodes():\n",
    "    cpd = model.get_cpds(node)\n",
    "    print(node)\n",
    "    print(cpd.state_names[node])\n",
    "    stati_validi[node] = cpd.state_names[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf4253-df8d-4d42-a0b9-94c390fef326",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stati_validi['WEIGHT2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665ffe3-1675-49b7-8a2e-f0a04397b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuetof = 1367.3837890625\n",
    "if valuetof in df_sampled['_LLCPWT2'].values:\n",
    "    print(\"presente\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeaf095-ce06-4b57-bb23-b6aaa41ed5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterare il dataframe a blocchi di 100 righe\n",
    "blocco = 0\n",
    "for start in range(blocco*100, len(df_test_data), 100):\n",
    "    end = start + 100\n",
    "    df_blocco = train_data.iloc[start:end]\n",
    "    print(f'Prediction per: {start}:{end}, blocco {blocco}')\n",
    "\n",
    "    \n",
    "    for index, row in df_blocco.iterrows():\n",
    "        print(f'Prediction for interrows: {index}, {start}:{end}, blocco {blocco}')\n",
    "\n",
    "        evidence = row.to_dict()\n",
    "        evidence.pop('SLEPTIM1')\n",
    "        #for node, value in evidence.items():\n",
    "            #cpd = model.get_cpds(node)\n",
    "            #valid_states = cpd.state_names[node]\n",
    "            #if value not in valid_states:\n",
    "                #raise ValueError(f\"Lo stato '{value}' non è valido per il nodo '{node}'\")\n",
    "        #filtered_evidence = {k: v for k, v in evidence.items() if k in model_nodes}\n",
    "        #print(filtered_evidence)\n",
    "        \n",
    "        try:\n",
    "            prediction = inference.map_query(variables=['DIABTYPE1'], evidence=evidence)\n",
    "            \n",
    "            # Debug: stampa il risultato della previsione\n",
    "            print(f'Prediction per index {index}: {prediction}')\n",
    "            \n",
    "            # Verifica il tipo di prediction\n",
    "            if isinstance(prediction, dict) and 'DIABTYPE1' in prediction:\n",
    "                y_pred.append(prediction['DIABTYPE1'])\n",
    "            else:\n",
    "                print(f'Predizione non valida per index {index}: {prediction}')\n",
    "                y_pred.append(None)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'Errore al processing dell\\'indice {index}: {e}')\n",
    "            y_pred.append(None)\n",
    "    \n",
    "    # Salvare le previsioni su file dopo ogni blocco di 100 righe\n",
    "    salva_previsioni(y_pred, blocco)\n",
    "    blocco += 1\n",
    "    y_pred = []  # Reset della lista per il prossimo blocco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324c236-2e8b-434d-b795-28c55a94607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rimuovere None dai predetti e dai veri valori corrispondenti\n",
    "y_true_filtered = [yt for yt, yp in zip(y_true, y_pred) if yp is not None]\n",
    "y_pred_filtered = [yp for yp in y_pred if yp is not None]\n",
    "\n",
    "# Calcolare le metriche di valutazione\n",
    "accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "precision = precision_score(y_true_filtered, y_pred_filtered, average='macro')  # O 'micro' o 'weighted' a seconda del tuo caso\n",
    "recall = recall_score(y_true_filtered, y_pred_filtered, average='macro')  # O 'micro' o 'weighted'\n",
    "f1 = f1_score(y_true_filtered, y_pred_filtered, average='macro')  # O 'micro' o 'weighted'\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df100252-5a53-4c88-b7c2-270adab8046e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabeteGruppo4",
   "language": "python",
   "name": "diabetegruppo4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
