# -*- coding: utf-8 -*-
"""TerzoApproccio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sdnRrw5Y74VMtfxyUm6vVGzp6Cmjvin9
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""Lettura del dataset e cleaning della variabile target


"""

# File già salvato con il drop delle righe NaN sulla colonna DIABTYPE
df = pd.read_sas('/content/LLCP2022.XPT ', format = "xport", encoding='utf-8')
print(df.head(5))

df[df.DIABTYPE.isna()]['DIABETE4'].value_counts(dropna=False)

df[df.DIABTYPE.isna()==False]['DIABTYPE'].value_counts(dropna=False)

df[(df.DIABTYPE.isna()==False) & (df.DIABTYPE!=7) & (df.DIABTYPE!=9)].to_csv("LLCP2022_clean.csv")

"""Inizio del preprocessing

Selezionare il campione di pazienti non malati di diabete
"""

df.DIABETE4.replace({1: 1, 2: 1, 3 : 0, 4 : 0}, inplace=True)

df.DIABETE4.value_counts(dropna=False)

df_sani = df[(df.DIABETE4.isna()==False) & (df.DIABETE4!=7) & (df.DIABETE4!=9) & (df.DIABETE4!=1)]

df_sani.DIABETE4.value_counts()

df_sani.DIABTYPE.value_counts(dropna=False)

"""Selezionare il campione di pazienti malati di diabete"""

df = pd.read_csv('LLCP2022_clean.csv')
df = df.drop('Unnamed: 0', axis=1)
print(df)

df['DIABTYPE1']=df.DIABTYPE.map(lambda x : 1 if x==1 else 0)

df['DIABTYPE2']=df.DIABTYPE.map(lambda x : 1 if x==2 else 0)

df[['DIABTYPE1', 'DIABTYPE2']].value_counts()

"""Distribuzione DIABTYPE1 e grafico"""

df['stratify_var'] = df._AGE_G.astype(str) + '_' + df.SEXVAR.astype(str)

grafico1 = df[['stratify_var', 'DIABTYPE1']].value_counts().reset_index()

grafico1 = grafico1[grafico1.DIABTYPE1==1].sort_values(by='stratify_var')[['count', 'stratify_var']]

import pandas as pd
import matplotlib.pyplot as plt

# Estraiamo il sesso dalla colonna 'stratify_var', considerando 1=Maschio e 2=Femmina
grafico1['sesso'] = grafico1['stratify_var'].apply(lambda x: 'Maschio' if x.split('_')[1] == '1.0' else 'Femmina')

# Definiamo i colori per maschi e femmine
colori = grafico1['sesso'].map({'Maschio': 'blue', 'Femmina': 'pink'})

# Creiamo il grafico a barre con i colori per sesso
plt.bar(grafico1['stratify_var'], grafico1['count'], color=colori)

# Aggiungiamo il titolo e le etichette degli assi
plt.title('Count by Stratify Variable and Sex DIABTYPE1')
plt.xlabel('Stratify Variable')
plt.ylabel('Count')

# Ruotiamo le etichette dell'asse delle X in verticale
plt.xticks(rotation=90)

# Rimuoviamo le spine superiore e destra
plt.gca().spines[['top', 'right']].set_visible(False)

# Creiamo la legenda
from matplotlib.lines import Line2D
legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Men'),
                   Line2D([0], [0], color='pink', lw=4, label='Women')]
plt.legend(handles=legend_elements)

# Mostriamo il grafico
plt.show()

"""Distribuzione DIABTYPE2 e grafico"""

grafico2 = df[['stratify_var', 'DIABTYPE2']].value_counts().reset_index()

grafico2 = grafico2[grafico2.DIABTYPE2==1].sort_values(by='stratify_var')[['count', 'stratify_var']]

# Estraiamo il sesso dalla colonna 'stratify_var', considerando 1=Maschio e 2=Femmina
grafico2['sesso'] = grafico2['stratify_var'].apply(lambda x: 'Maschio' if x.split('_')[1] == '1.0' else 'Femmina')

# Definiamo i colori per maschi e femmine
colori = grafico2['sesso'].map({'Maschio': 'blue', 'Femmina': 'pink'})

# Creiamo il grafico a barre con i colori per sesso
plt.bar(grafico2['stratify_var'], grafico2['count'], color=colori)

# Aggiungiamo il titolo e le etichette degli assi
plt.title('Count by Stratify Variable and Sex DIABTYPE2')
plt.xlabel('Stratify Variable')
plt.ylabel('Count')

# Ruotiamo le etichette dell'asse delle X in verticale
plt.xticks(rotation=90)

# Rimuoviamo le spine superiore e destra
plt.gca().spines[['top', 'right']].set_visible(False)

# Creiamo la legenda
from matplotlib.lines import Line2D
legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Men'),
                   Line2D([0], [0], color='pink', lw=4, label='Women')]
plt.legend(handles=legend_elements)

# Mostriamo il grafico
plt.show()

"""Campione stratificato

Abbiamo un dataset con 1.000 soggetti affetti da diabete di tipo 1 e 10.000 soggetti con diabete di tipo 2. Vogliamo creare un nuovo dataset in cui la distribuzione di età (una variabile categoriale con 6 categorie) e sesso nei soggetti con diabete di tipo 2 sia simile a quella dei soggetti con diabete di tipo 1.

Passaggi:

Dividere i dati: Separiamo i dati dei soggetti con diabete di tipo 1 e diabete di tipo 2 in due gruppi distinti.

Calcolare le distribuzioni: Analizziamo il gruppo con diabete di tipo 1 per capire come si distribuiscono le variabili sesso e età. Per ogni combinazione di sesso ed età (ad esempio, maschi tra i 21-30 anni), contiamo quanti soggetti ci sono.

Sottocampionare il gruppo con diabete di tipo 2: Preleviamo un campione dal gruppo di diabete di tipo 2 che corrisponde a ogni combinazione di sesso ed età, in modo che abbia lo stesso numero di soggetti della corrispondente combinazione nel gruppo con diabete di tipo 1. Se nel gruppo con diabete di tipo 2 ci sono più soggetti di quelli necessari, ne estraiamo casualmente un numero pari a quello del gruppo tipo 1. Se ci sono meno soggetti, li prendiamo tutti.

Combinare i dati: Una volta effettuato il sottocampionamento del gruppo con diabete di tipo 2, uniamo i dati campionati con quelli del gruppo con diabete di tipo 1.

Risultato:

Alla fine, otteniamo un nuovo dataset in cui il gruppo con diabete di tipo 2 (che originariamente aveva 10.000 soggetti) è ridotto, ma ha una distribuzione di età e sesso simile a quella del gruppo con diabete di tipo 1 (1.000 soggetti).

Questo nuovo dataset può essere usato per analisi o confronti tra i due gruppi, sapendo che hanno distribuzioni simili per queste due variabili chiave.
"""

import pandas as pd

# Supponiamo di avere un dataframe `df` con le seguenti colonne:
# - 'diabete': variabile che indica il tipo di diabete (1 per tipo 1, 2 per tipo 2)
# - 'sesso': variabile categoriale (M/F)
# - 'eta': variabile categoriale con 6 categorie (es. '0-10', '11-20', '21-30', etc.)

# Dividiamo i dati per diabete di tipo 1 e tipo 2
df_tipo1 = df[df['DIABTYPE1'] == 1]
df_tipo2 = df[df['DIABTYPE2'] == 1]

# Verifichiamo la distribuzione di sesso ed età nel gruppo con diabete di tipo 1
distribuzione_tipo1 = df_tipo1.groupby(['SEXVAR', '_AGE_G']).size().reset_index(name='counts')

# Effettuiamo il sottocampionamento stratificato dal gruppo con diabete di tipo 2
df_tipo2_sampled = pd.DataFrame()

# Per ogni combinazione di sesso ed età nel gruppo tipo 1, estraiamo un numero corrispondente di soggetti dal gruppo tipo 2
for index, row in distribuzione_tipo1.iterrows():
    sesso = row['SEXVAR']
    eta = row['_AGE_G']
    count = row['counts']

    # Filtriamo il gruppo con diabete di tipo 2 per sesso ed età
    subset_tipo2 = df_tipo2[(df_tipo2['SEXVAR'] == sesso) & (df_tipo2['_AGE_G'] == eta)]

    # Se il numero di soggetti nel gruppo tipo 2 è maggiore o uguale a `count`, estraiamo un campione di dimensione `count`
    if len(subset_tipo2) >= count:
        sampled_subset = subset_tipo2.sample(n=int(count), random_state=42)
    else:
        # Se non ci sono abbastanza soggetti, prendiamo tutti quelli disponibili
        sampled_subset = subset_tipo2

    # Aggiungiamo i campioni estratti al dataframe finale
    df_tipo2_sampled = pd.concat([df_tipo2_sampled, sampled_subset])

# Combiniamo il gruppo tipo 1 e il gruppo tipo 2 campionato
df_balanced = pd.concat([df_tipo1, df_tipo2_sampled])

# Verifichiamo le dimensioni del nuovo dataset
print(df_balanced.shape)

# Ora `df_balanced` contiene una distribuzione per età e sesso nel gruppo diabete di tipo 2 simile a quella del gruppo tipo 1.

grafico3 = df_tipo2_sampled[['stratify_var', 'DIABTYPE2']].value_counts().reset_index() #CAMBIATO DA df_balanced a df_tipo2_sampled

grafico3 = grafico3[grafico3.DIABTYPE2==1].sort_values(by='stratify_var')[['count', 'stratify_var']]

# Estraiamo il sesso dalla colonna 'stratify_var', considerando 1=Maschio e 2=Femmina
grafico3['sesso'] = grafico3['stratify_var'].apply(lambda x: 'Maschio' if x.split('_')[1] == '1.0' else 'Femmina')

# Definiamo i colori per maschi e femmine
colori = grafico3['sesso'].map({'Maschio': 'blue', 'Femmina': 'pink'})

# Creiamo il grafico a barre con i colori per sesso
plt.bar(grafico3['stratify_var'], grafico3['count'], color=colori)

# Aggiungiamo il titolo e le etichette degli assi
plt.title('Count by Stratify Variable and Sex DIABTYPE2 after sampling')
plt.xlabel('Stratify Variable')
plt.ylabel('Count')

# Ruotiamo le etichette dell'asse delle X in verticale
plt.xticks(rotation=90)

# Rimuoviamo le spine superiore e destra
plt.gca().spines[['top', 'right']].set_visible(False)

# Creiamo la legenda
from matplotlib.lines import Line2D
legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Men'),
                   Line2D([0], [0], color='pink', lw=4, label='Women')]
plt.legend(handles=legend_elements)

# Mostriamo il grafico
plt.show()

df_sani['stratify_var'] = df_sani._AGE_G.astype(str) + '_' + df_sani.SEXVAR.astype(str)

grafico4 = df_sani[['stratify_var']].value_counts().reset_index()

grafico4 = grafico4.sort_values(by='stratify_var')[['count', 'stratify_var']]

import pandas as pd
import matplotlib.pyplot as plt

# Estraiamo il sesso dalla colonna 'stratify_var', considerando 1=Maschio e 2=Femmina
grafico4['sesso'] = grafico4['stratify_var'].apply(lambda x: 'Maschio' if x.split('_')[1] == '1.0' else 'Femmina')

# Definiamo i colori per maschi e femmine
colori = grafico4['sesso'].map({'Maschio': 'blue', 'Femmina': 'pink'})

# Creiamo il grafico a barre con i colori per sesso
plt.bar(grafico4['stratify_var'], grafico4['count'], color=colori)

# Aggiungiamo il titolo e le etichette degli assi
plt.title('Count by Stratify Variable and Sex for healthy population')
plt.xlabel('Stratify Variable')
plt.ylabel('Count')

# Ruotiamo le etichette dell'asse delle X in verticale
plt.xticks(rotation=90)

# Rimuoviamo le spine superiore e destra
plt.gca().spines[['top', 'right']].set_visible(False)

# Creiamo la legenda
from matplotlib.lines import Line2D
legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Men'),
                   Line2D([0], [0], color='pink', lw=4, label='Women')]
plt.legend(handles=legend_elements)

# Mostriamo il grafico
plt.show()

"""Campionamento per sesso ed età sui pazienti sani"""

import pandas as pd

# Supponiamo di avere un dataframe `df` con le seguenti colonne:
# - 'diabete': variabile che indica il tipo di diabete (1 per tipo 1, 2 per tipo 2)
# - 'sesso': variabile categoriale (M/F)
# - 'eta': variabile categoriale con 6 categorie (es. '0-10', '11-20', '21-30', etc.)

# Dividiamo i dati per diabete di tipo 1 e tipo 2
df_tipo1 = df[df['DIABTYPE1'] == 1]
df_tipo2 = df_sani

# Verifichiamo la distribuzione di sesso ed età nel gruppo con diabete di tipo 1
distribuzione_tipo1 = df_tipo1.groupby(['SEXVAR', '_AGE_G']).size().reset_index(name='counts')

# Effettuiamo il sottocampionamento stratificato dal gruppo con diabete di tipo 2
df_sani_sampled = pd.DataFrame()

# Per ogni combinazione di sesso ed età nel gruppo tipo 1, estraiamo un numero corrispondente di soggetti dal gruppo tipo 2
for index, row in distribuzione_tipo1.iterrows():
    sesso = row['SEXVAR']
    eta = row['_AGE_G']
    count = row['counts']

    # Filtriamo il gruppo con diabete di tipo 2 per sesso ed età
    subset_tipo2 = df_tipo2[(df_tipo2['SEXVAR'] == sesso) & (df_tipo2['_AGE_G'] == eta)]

    # Se il numero di soggetti nel gruppo tipo 2 è maggiore o uguale a `count`, estraiamo un campione di dimensione `count`
    if len(subset_tipo2) >= count:
        sampled_subset = subset_tipo2.sample(n=int(count), random_state=42)
    else:
        # Se non ci sono abbastanza soggetti, prendiamo tutti quelli disponibili
        sampled_subset = subset_tipo2

    # Aggiungiamo i campioni estratti al dataframe finale
    df_sani_sampled = pd.concat([df_sani_sampled, sampled_subset])

# Combiniamo il gruppo tipo 1 e il gruppo tipo 2 campionato
# df_balanced = pd.concat([df_tipo1, df_sani_sampled])

# Verifichiamo le dimensioni del nuovo dataset
print(df_sani_sampled.shape)

grafico5 = df_sani_sampled[['stratify_var']].value_counts().reset_index()

grafico5 = grafico5.sort_values(by='stratify_var')[['count', 'stratify_var']]

# Estraiamo il sesso dalla colonna 'stratify_var', considerando 1=Maschio e 2=Femmina
grafico5['sesso'] = grafico5['stratify_var'].apply(lambda x: 'Maschio' if x.split('_')[1] == '1.0' else 'Femmina')

# Definiamo i colori per maschi e femmine
colori = grafico5['sesso'].map({'Maschio': 'blue', 'Femmina': 'pink'})

# Creiamo il grafico a barre con i colori per sesso
plt.bar(grafico5['stratify_var'], grafico5['count'], color=colori)

# Aggiungiamo il titolo e le etichette degli assi
plt.title('Count by Stratify Variable and Sex for healthy population after sampling')
plt.xlabel('Stratify Variable')
plt.ylabel('Count')

# Ruotiamo le etichette dell'asse delle X in verticale
plt.xticks(rotation=90)

# Rimuoviamo le spine superiore e destra
plt.gca().spines[['top', 'right']].set_visible(False)

# Creiamo la legenda
from matplotlib.lines import Line2D
legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Men'),
                   Line2D([0], [0], color='pink', lw=4, label='Women')]
plt.legend(handles=legend_elements)

# Mostriamo il grafico
plt.show()

"""Features Selection"""

df_diabetype1 = df[df.DIABTYPE1 == 1]
#df_diabetype2 = df_balanced
df_diabetype2 = df_tipo2_sampled

print("Per diabete di tipo 1 abbiamo il seguente dataframe:", df_diabetype1.shape)
print("Per diabete di tipo 2 abbiamo il seguente dataframe:", df_diabetype2.shape)
print("Per una popolazione sana abbiamo il seguente dataframe:", df_sani_sampled.shape)

df_total1 = pd.concat([df_diabetype1, df_sani_sampled])

df_total2 = pd.concat([df_diabetype2, df_sani_sampled])

df_total1 = df_total1.drop(columns = ['DIABTYPE2'])

df_total2 = df_total2.drop(columns = ['DIABTYPE1'])

df_total1.DIABTYPE1.replace(np.nan, 0, inplace=True)
df_total1[['DIABETE4', 'DIABTYPE1']].value_counts(dropna=False)

df_total2.DIABTYPE2.replace(np.nan, 0, inplace=True)
df_total2[['DIABETE4', 'DIABTYPE2']].value_counts(dropna=False)

"""Eliminiamo dal dataframe le colonne che hanno più dell'70% di missing values

Eliminiamo le colonne 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'SEQNO', 'FMONTH' perché esprimono variabili temporali non utili ai fini dell'analisi
"""

# Soglia per la percentuale di valori mancanti
threshold = 0.7

# Calcola la percentuale di valori mancanti per ogni colonna
missing_percentages1 = df_total1.isnull().mean()

# Seleziona le colonne con percentuale di missing inferiore all'70%
cols_to_keep1 = missing_percentages1[missing_percentages1 < threshold].index

# Crea un nuovo DataFrame con solo le colonne da mantenere
df_cleaned1 = df_total1[cols_to_keep1]

# Output: mostriamo le colonne eliminate
cols_eliminated1 = missing_percentages1[missing_percentages1 >= threshold].index
print("Numero colonne eliminate:", len(list(cols_eliminated1)))
print("Colonne eliminate:", list(cols_eliminated1))

df_cleaned1 = df_cleaned1.drop(columns = ['IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'SEQNO', 'FMONTH', 'DIABAGE4', 'DIABTYPE', 'stratify_var'])

print("Abbiamo il seguente dataframe:", df_cleaned1.shape)

# Soglia per la percentuale di valori mancanti
threshold = 0.7

# Calcola la percentuale di valori mancanti per ogni colonna
missing_percentages2 = df_total2.isnull().mean()

# Seleziona le colonne con percentuale di missing inferiore all'70%
cols_to_keep2 = missing_percentages2[missing_percentages2 < threshold].index

# Crea un nuovo DataFrame con solo le colonne da mantenere
df_cleaned2 = df_total2[cols_to_keep2]

# Output: mostriamo le colonne eliminate
cols_eliminated2 = missing_percentages2[missing_percentages2 >= threshold].index
print("Numero colonne eliminate:", len(list(cols_eliminated2)))
print("Colonne eliminate:", list(cols_eliminated2))

df_cleaned2 = df_cleaned2.drop(columns = ['IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'SEQNO', 'FMONTH', 'DIABAGE4', 'DIABTYPE', 'stratify_var'])

print("Abbiamo il seguente dataframe:", df_cleaned2.shape)

df_cleaned1.to_csv("LLCP2022_cleaned1.csv")

df_cleaned2.to_csv("LLCP2022_cleaned2.csv")

df_cleaned1 = pd.read_csv('LLCP2022_cleaned1.csv')
df_cleaned1 = df_cleaned1.drop('Unnamed: 0', axis=1)

df_cleaned2 = pd.read_csv('LLCP2022_cleaned2.csv')
df_cleaned2 = df_cleaned2.drop('Unnamed: 0', axis=1)

"""Correlazione Pearson"""

# Sostituiamo '7', '9' e 'NaN' con -1
df_cleaned1.replace({7: -1, 9: -1, 77 : -1, 99 : -1, 777: -1, 999: -1, 7777: -1, 9999: -1, 'nan': -1}, inplace=True)
# Poiché 'NaN' potrebbe essere interpretato come valore NaN e non come stringa 'NaN', sostituiamo anche i valori np.nan
df_cleaned1.replace(np.nan, -1, inplace=True)

# Sostituiamo '7', '9' e 'NaN' con -1
df_cleaned2.replace({7: -1, 9: -1, 77 : -1, 99 : -1, 777: -1, 999: -1, 7777: -1, 9999: -1, 'nan': -1}, inplace=True)
# Poiché 'NaN' potrebbe essere interpretato come valore NaN e non come stringa 'NaN', sostituiamo anche i valori np.nan
df_cleaned2.replace(np.nan, -1, inplace=True)

from scipy.stats import pearsonr

# Supponendo che il DataFrame si chiami df
# La colonna target si chiama 'DIABTYPE1'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned1.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Pearson e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE1':
        corr, p_value = pearsonr(df_cleaned1[column], df_cleaned1['DIABTYPE1'])
        results.append((column, corr, p_value))


# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_pearson1 = pd.DataFrame(results, columns=['Variable', 'Pearson_Correlation', 'P_Value'])

print(corr_df_pearson1)

corr_df_sort_pearson1 = corr_df_pearson1.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index']).reset_index()

corr_df_sort_pearson1[corr_df_sort_pearson1.P_Value < 0.01]

from scipy.stats import pearsonr

# Supponendo che il DataFrame si chiami df
# La colonna target si chiama 'DIABTYPE2'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned2.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Pearson e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE2':
        corr, p_value = pearsonr(df_cleaned2[column], df_cleaned2['DIABTYPE2'])
        results.append((column, corr, p_value))


# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_pearson2 = pd.DataFrame(results, columns=['Variable', 'Pearson_Correlation', 'P_Value'])

print(corr_df_pearson2)

corr_df_sort_pearson2 = corr_df_pearson2.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index']).reset_index()

corr_df_sort_pearson2[corr_df_sort_pearson2.P_Value < 0.01]

lista_variabile_person1 = corr_df_sort_pearson1[corr_df_sort_pearson1.P_Value < 0.01].Variable.to_list()

lista_variabile_person2 = corr_df_sort_pearson2[corr_df_sort_pearson2.P_Value < 0.01].Variable.to_list()

elementi_comuni_person = list(set(lista_variabile_person1) & set(lista_variabile_person2))
print("Numero elementi comuni:", len(elementi_comuni_person))
print("Elementi comuni:", elementi_comuni_person)

"""Correlazione Spearman"""

from scipy.stats import spearmanr

# Supponendo che il DataFrame si chiami df_cleaned e la colonna target sia 'DIABTYPE1'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned1.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Spearman e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE1':
        corr, p_value = spearmanr(df_cleaned1[column], df_cleaned1['DIABTYPE1'])
        results.append((column, corr, p_value))

# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_spearman1 = pd.DataFrame(results, columns=['Variable', 'Spearman_Correlation', 'P_Value'])

print(corr_df_spearman1)

corr_df_sort_spearman1 = corr_df_spearman1.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index']).reset_index()

corr_df_sort_spearman1[corr_df_sort_spearman1.P_Value < 0.01]

from scipy.stats import spearmanr

# Supponendo che il DataFrame si chiami df_cleaned e la colonna target sia 'DIABTYPE2'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned2.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Spearman e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE2':
        corr, p_value = spearmanr(df_cleaned2[column], df_cleaned2['DIABTYPE2'])
        results.append((column, corr, p_value))

# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_spearman2 = pd.DataFrame(results, columns=['Variable', 'Spearman_Correlation', 'P_Value'])

print(corr_df_spearman2)

corr_df_sort_spearman2 = corr_df_spearman2.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index']).reset_index()

corr_df_sort_spearman2[corr_df_sort_spearman2.P_Value < 0.01]

lista_variabile_spearman1 = corr_df_sort_spearman1[corr_df_sort_spearman1.P_Value < 0.01].Variable.to_list()

lista_variabile_spearman2 = corr_df_sort_spearman2[corr_df_sort_spearman2.P_Value < 0.01].Variable.to_list()

elementi_comuni_spearman = list(set(lista_variabile_spearman1) & set(lista_variabile_spearman2))
print("Numero elementi comuni:", len(elementi_comuni_spearman))
print("Elementi comuni:", elementi_comuni_spearman)

"""------ Random Forest ------"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# plotly
# import plotly.plotly as py
import plotly.express as px
from plotly.offline import init_notebook_mode, iplot, plot
import plotly as py
init_notebook_mode(connected=True)
import plotly.graph_objs as go

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

import pickle

# for reproducibility
random_state_nr = 42

Y = df_cleaned1['DIABTYPE1']
X = df_cleaned1.drop(['DIABTYPE1'], axis=1)

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True,random_state = 42)
# Print sizes
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

clf = RandomForestClassifier(random_state = 42, n_estimators=10, max_depth=10, n_jobs=-1)
clf.fit(X_train, y_train)

# predict the test set
y_pred = clf.predict(X_test)

# calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

n_features = len(X.columns)
# plot the feature importance
plt.figure(figsize=(5,10))
feat_importances1 = pd.Series(clf.feature_importances_, index=X.columns)
# the same plot but with seaborn and features in the x axis
sns.barplot(x=feat_importances1.nlargest(40), y=feat_importances1.nlargest(40).index)
plt.title('Random Forest - Accuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))
# name the x-axis
plt.xlabel('Mean decrease in impurity')

plt.show()

Y = df_cleaned2['DIABTYPE2']
X = df_cleaned2.drop(['DIABTYPE2'], axis=1)

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True,random_state = 42)
# Print sizes
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

clf = RandomForestClassifier(random_state = 42, n_estimators=10, max_depth=10, n_jobs=-1)
clf.fit(X_train, y_train)

# predict the test set
y_pred = clf.predict(X_test)

# calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

n_features = len(X.columns)
# plot the feature importance
plt.figure(figsize=(5,10))
feat_importances2 = pd.Series(clf.feature_importances_, index=X.columns)
# the same plot but with seaborn and features in the x axis
sns.barplot(x=feat_importances2.nlargest(40), y=feat_importances2.nlargest(40).index)
plt.title('Random Forest - Accuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))
# name the x-axis
plt.xlabel('Mean decrease in impurity')

plt.show()

feat_importances1.reset_index().reset_index()

random_features1 = feat_importances1[feat_importances1 > 0]
random_features1 = random_features1.sort_values(ascending=False)
random_features1 = list(random_features1.index)

random_features2 = feat_importances2[feat_importances2 > 0]
random_features2 = random_features2.sort_values(ascending=False)
random_features2 = list(random_features2.index)

elementi_comuni_diabete1 = list(set(lista_variabile_person1) & set(lista_variabile_spearman1) & set(random_features1))
print("Numero elementi comuni per diabete1:", len(elementi_comuni_diabete1))
print("Elementi comuni per diabete1:", elementi_comuni_diabete1)

elementi_comuni_diabete2 = list(set(lista_variabile_person2) & set(lista_variabile_spearman2) & set(random_features2))
print("Numero elementi comuni per diabete2:", len(elementi_comuni_diabete2))
print("Elementi comuni per diabete2:", elementi_comuni_diabete2)

tmp_pear1 = corr_df_sort_pearson1[corr_df_sort_pearson1.Variable.isin(elementi_comuni_diabete1)].drop(columns= 'index').reset_index().reset_index().drop(columns= 'index').rename(columns= {'level_0': 'index'})
tmp_pear1['type'] = 'Pearson'
tmp_spear1 = corr_df_sort_spearman1[corr_df_sort_spearman1.Variable.isin(elementi_comuni_diabete1)].drop(columns= 'index').reset_index().reset_index().drop(columns= 'index').rename(columns= {'level_0': 'index'})
tmp_spear1['type'] = 'Spearman'
tmp_rand1 = feat_importances1.reset_index().reset_index()
tmp_rand1['type'] = 'Random'

tmp_rand1 = tmp_rand1.rename(columns= {'level_0': 'index', 'index': 'Variable'})

tmp_rand1 = tmp_rand1[tmp_rand1.Variable.isin(elementi_comuni_diabete1)].drop(columns= 'index').reset_index().reset_index().drop(columns= 'index').rename(columns= {'level_0': 'index'})

df_features = pd.concat([tmp_pear1[['index', 'Variable', 'type']], tmp_spear1[['index', 'Variable', 'type']], tmp_rand1[['index', 'Variable', 'type']]])

df_features = pd.pivot_table(df_features, values='index', index='Variable', columns='type').reset_index()

df_features['rank'] = df_features.iloc[:,1:4].mean(axis=1)

features_diab1 = list(df_features.sort_values(by='rank', ascending=True)[:20].Variable)

features_diab1.append('DIABTYPE1')

features_diab1

df_diabete1 = df_cleaned1[features_diab1]
df_diabete1.to_csv("T1DM.csv")

"""------------ TDM2

"""

tmp_pear2 = corr_df_sort_pearson2[corr_df_sort_pearson2.Variable.isin(elementi_comuni_diabete2)].drop(columns= 'index').reset_index().reset_index().drop(columns= 'index').rename(columns= {'level_0': 'index'})
tmp_pear2['type'] = 'Pearson'
tmp_spear2 = corr_df_sort_spearman2[corr_df_sort_spearman2.Variable.isin(elementi_comuni_diabete2)].drop(columns= 'index').reset_index().reset_index().drop(columns= 'index').rename(columns= {'level_0': 'index'})
tmp_spear2['type'] = 'Spearman'
tmp_rand2 = feat_importances2.reset_index().reset_index()
tmp_rand2['type'] = 'Random'

tmp_rand2 = tmp_rand2.rename(columns= {'level_0': 'index', 'index': 'Variable'})

tmp_rand2 = tmp_rand2[tmp_rand2.Variable.isin(elementi_comuni_diabete2)].drop(columns= 'index').reset_index().reset_index().drop(columns= 'index').rename(columns= {'level_0': 'index'})

df_features2 = pd.concat([tmp_pear2[['index', 'Variable', 'type']], tmp_spear2[['index', 'Variable', 'type']], tmp_rand2[['index', 'Variable', 'type']]])

df_features2 = pd.pivot_table(df_features2, values='index', index='Variable', columns='type').reset_index()

df_features2['rank'] = df_features2.iloc[:,1:4].mean(axis=1)

features_diab2 = list(df_features2.sort_values(by='rank', ascending=True)[:20].Variable)

features_diab2.append('DIABTYPE2')

features_diab2

df_diabete2 = df_cleaned2[features_diab2]
df_diabete2.to_csv("T2DM.csv")

"""Bayesian Network"""









"""------------------------------------"""

df_total = pd.concat([df_diabetype1, df_diabetype2])

# Soglia per la percentuale di valori mancanti
threshold = 0.7

# Calcola la percentuale di valori mancanti per ogni colonna
missing_percentages = df_total.isnull().mean()

# Seleziona le colonne con percentuale di missing inferiore all'70%
cols_to_keep = missing_percentages[missing_percentages < threshold].index

# Crea un nuovo DataFrame con solo le colonne da mantenere
df_cleaned = df_total[cols_to_keep]

# Output: mostriamo le colonne eliminate
cols_eliminated = missing_percentages[missing_percentages >= threshold].index
print("Numero colonne eliminate:", len(list(cols_eliminated)))
print("Colonne eliminate:", list(cols_eliminated))

df_cleaned = df_cleaned.drop(columns = ['IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'SEQNO', 'FMONTH', 'DIABAGE4', 'DIABTYPE', 'stratify_var'])

print("Abbiamo il seguente dataframe:", df_cleaned.shape)

"""Correlazione Pearson"""

# Sostituiamo '7', '9' e 'NaN' con -1
df_cleaned.replace({7: -1, 9: -1, 77 : -1, 99 : -1, 777: -1, 999: -1, 7777: -1, 9999: -1, 'nan': -1}, inplace=True)
# Poiché 'NaN' potrebbe essere interpretato come valore NaN e non come stringa 'NaN', sostituiamo anche i valori np.nan
df_cleaned.replace(np.nan, -1, inplace=True)

from scipy.stats import pearsonr

# Supponendo che il DataFrame si chiami df
# La colonna target si chiama 'DIABTYPE1'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Pearson e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE1':
        corr, p_value = pearsonr(df_cleaned[column], df_cleaned['DIABTYPE1'])
        results.append((column, corr, p_value))


# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_pearson1 = pd.DataFrame(results, columns=['Variable', 'Pearson_Correlation', 'P_Value'])

print(corr_df_pearson1)

corr_df_sort_pearson1 = corr_df_pearson1.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index'])

corr_df_sort_pearson1[corr_df_sort_pearson1.P_Value < 0.05]

from scipy.stats import pearsonr

# Supponendo che il DataFrame si chiami df
# La colonna target si chiama 'DIABTYPE2'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Pearson e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE2':
        corr, p_value = pearsonr(df_cleaned[column], df_cleaned['DIABTYPE2'])
        results.append((column, corr, p_value))


# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_pearson2 = pd.DataFrame(results, columns=['Variable', 'Pearson_Correlation', 'P_Value'])

print(corr_df_pearson2)

corr_df_sort_pearson2 = corr_df_pearson2.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index'])

corr_df_sort_pearson2[corr_df_sort_pearson2.P_Value < 0.05]

"""Correlazione con Spearman"""

from scipy.stats import spearmanr

# Supponendo che il DataFrame si chiami df_cleaned e la colonna target sia 'DIABTYPE1'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Spearman e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE1':
        corr, p_value = spearmanr(df_cleaned[column], df_cleaned['DIABTYPE1'])
        results.append((column, corr, p_value))

# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_spearman1 = pd.DataFrame(results, columns=['Variable', 'Spearman_Correlation', 'P_Value'])

print(corr_df_spearman1)

corr_df_sort_spearman1 = corr_df_spearman1.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index'])

corr_df_sort_spearman1[corr_df_sort_spearman1.P_Value < 0.05]

from scipy.stats import spearmanr

# Supponendo che il DataFrame si chiami df_cleaned e la colonna target sia 'DIABTYPE2'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Spearman e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE2':
        corr, p_value = spearmanr(df_cleaned[column], df_cleaned['DIABTYPE2'])
        results.append((column, corr, p_value))

# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_spearman2 = pd.DataFrame(results, columns=['Variable', 'Spearman_Correlation', 'P_Value'])

print(corr_df_spearman2)

corr_df_sort_spearman2 = corr_df_spearman2.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index'])

corr_df_sort_spearman2[corr_df_sort_spearman2.P_Value < 0.05]

"""Random Forest"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# plotly
# import plotly.plotly as py
import plotly.express as px
from plotly.offline import init_notebook_mode, iplot, plot
import plotly as py
init_notebook_mode(connected=True)
import plotly.graph_objs as go

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

import pickle

# for reproducibility
random_state_nr = 42

Y = df_cleaned['DIABTYPE1']
X = df_cleaned.drop(['DIABTYPE1'], axis=1)

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True,random_state = 42)
# Print sizes
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

clf = RandomForestClassifier(random_state = 42, n_estimators=10, max_depth=10, n_jobs=-1)
clf.fit(X_train, y_train)

# predict the test set
y_pred = clf.predict(X_test)

# calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

n_features = len(X.columns)
# plot the feature importance
plt.figure(figsize=(5,10))
feat_importances = pd.Series(clf.feature_importances_, index=X.columns)
# the same plot but with seaborn and features in the x axis
sns.barplot(x=feat_importances.nlargest(40), y=feat_importances.nlargest(40).index)
plt.title('Random Forest - Accuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))
# name the x-axis
plt.xlabel('Mean decrease in impurity')

plt.show()

Y = df_cleaned['DIABTYPE2']
X = df_cleaned.drop(['DIABTYPE2'], axis=1)

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True,random_state = 42)
# Print sizes
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

clf = RandomForestClassifier(random_state = 42, n_estimators=10, max_depth=10, n_jobs=-1)
clf.fit(X_train, y_train)

# predict the test set
y_pred = clf.predict(X_test)

# calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

n_features = len(X.columns)
# plot the feature importance
plt.figure(figsize=(5,10))
feat_importances = pd.Series(clf.feature_importances_, index=X.columns)
# the same plot but with seaborn and features in the x axis
sns.barplot(x=feat_importances.nlargest(40), y=feat_importances.nlargest(40).index)
plt.title('Random Forest - Accuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))
# name the x-axis
plt.xlabel('Mean decrease in impurity')

plt.show()

"""Correlazione e Random Forest sul totale"""

df_total = pd.concat([df_diabetype1, df_diabetype2])

# Calcola la percentuale di valori mancanti per ogni colonna
missing_percentages = df_total.isnull().mean()

# Seleziona le colonne con percentuale di missing inferiore all'80%
cols_to_keep = missing_percentages[missing_percentages < threshold].index

# Crea un nuovo DataFrame con solo le colonne da mantenere
df_cleaned = df_total[cols_to_keep]

# Output: mostriamo le colonne eliminate
cols_eliminated = missing_percentages[missing_percentages >= threshold].index
print("Numero colonne eliminate:", len(list(cols_eliminated)))
print("Colonne eliminate:", list(cols_eliminated))

df_cleaned = df_cleaned.drop(columns = ['IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'SEQNO', 'FMONTH', 'DIABAGE4', 'DIABTYPE1', 'DIABTYPE2', 'stratify_var'])

print("Abbiamo il seguente dataframe:", df_cleaned.shape)

# Sostituiamo '7', '9' e 'NaN' con -1
df_cleaned.replace({7: -1, 9: -1, 77 : -1, 99 : -1, 777: -1, 999: -1, 7777: -1, 9999: -1, 'nan': -1}, inplace=True)
# Poiché 'NaN' potrebbe essere interpretato come valore NaN e non come stringa 'NaN', sostituiamo anche i valori np.nan
df_cleaned.replace(np.nan, -1, inplace=True)

from scipy.stats import pearsonr

# Supponendo che il DataFrame si chiami df
# La colonna target si chiama 'DIABTYPE'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Pearson e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE':
        corr, p_value = pearsonr(df_cleaned[column], df_cleaned['DIABTYPE'])
        results.append((column, corr, p_value))


# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_pearson = pd.DataFrame(results, columns=['Variable', 'Pearson_Correlation', 'P_Value'])

print(corr_df_pearson)

corr_df_sort_pearson = corr_df_pearson.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index'])

corr_df_sort_pearson[corr_df_sort_pearson.P_Value < 0.05]

from scipy.stats import spearmanr

# Supponendo che il DataFrame si chiami df_cleaned e la colonna target sia 'DIABTYPE'

# Lista per memorizzare i risultati
results = []

# Filtriamo solo le colonne numeriche
numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Calcoliamo la correlazione di Spearman e il p-value per ogni colonna numerica
for column in numeric_columns:
    # Escludiamo la colonna target stessa
    if column != 'DIABTYPE':
        corr, p_value = spearmanr(df_cleaned[column], df_cleaned['DIABTYPE'])
        results.append((column, corr, p_value))

# Trasformiamo i risultati in un DataFrame per una migliore leggibilità
corr_df_spearman = pd.DataFrame(results, columns=['Variable', 'Spearman_Correlation', 'P_Value'])

print(corr_df_spearman)

corr_df_sort_spearman = corr_df_spearman.sort_values(by="P_Value", ascending=True).reset_index().drop(columns = ['index'])

corr_df_sort_spearman[corr_df_sort_spearman.P_Value < 0.05]

Y = df_cleaned['DIABTYPE']
X = df_cleaned.drop(['DIABTYPE'], axis=1)

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True,random_state = 42)
# Print sizes
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

clf = RandomForestClassifier(random_state = 42, n_estimators=10, max_depth=10, n_jobs=-1)
clf.fit(X_train, y_train)

# predict the test set
y_pred = clf.predict(X_test)

# calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

n_features = len(X.columns)
# plot the feature importance
plt.figure(figsize=(5,10))
feat_importances = pd.Series(clf.feature_importances_, index=X.columns)
# the same plot but with seaborn and features in the x axis
sns.barplot(x=feat_importances.nlargest(40), y=feat_importances.nlargest(40).index)
plt.title('Random Forest - Accuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))
# name the x-axis
plt.xlabel('Mean decrease in impurity')

plt.show()